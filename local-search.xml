<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>hivesql语法</title>
    <link href="/2024/12/18/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Hive/hivesql%E8%AF%AD%E6%B3%95/"/>
    <url>/2024/12/18/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Hive/hivesql%E8%AF%AD%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="Hive-语法："><a href="#Hive-语法：" class="headerlink" title="Hive 语法："></a>Hive 语法：</h1><p>1.coalesce(a,b,c,d,e)求多个字段第一个非null值，若都为null则为null</p><p>2.静态分区动态分区插入</p><ul><li>静态分区和动态分区创建时一致</li><li>静态分区  insert into table partition(pt&#x3D;xxxxx)在插入时指定分区字段值</li><li>动态分区 insert Into table partition(pt) pt由后续select产生</li></ul><p>3.get_json_object(data,‘$.owner’)</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br><span class="hljs-attr">&quot;store&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br><span class="hljs-attr">&quot;fruit&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;weight&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">8</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;apple&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;weight&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">9</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;pear&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;bicycle&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;price&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">19.95</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;color&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;red&quot;</span><span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;email&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;amy@only_for_json_udf_test.net&quot;</span><span class="hljs-punctuation">,</span><br><span class="hljs-attr">&quot;owner&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;amy&quot;</span><br><span class="hljs-punctuation">&#125;</span><br>结果为amy<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hivesql语法</title>
    <link href="/2024/12/18/hivesql%E8%AF%AD%E6%B3%95/"/>
    <url>/2024/12/18/hivesql%E8%AF%AD%E6%B3%95/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>kafka八股</title>
    <link href="/2024/12/17/kafka%E5%85%AB%E8%82%A1/"/>
    <url>/2024/12/17/kafka%E5%85%AB%E8%82%A1/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数仓项目</title>
    <link href="/2024/12/17/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/%E6%95%B0%E4%BB%93/%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE/"/>
    <url>/2024/12/17/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/%E6%95%B0%E4%BB%93/%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="数仓项目"><a href="#数仓项目" class="headerlink" title="数仓项目"></a>数仓项目</h1><p>题目：电商交易域模型建设&amp;用户主题资产建设</p><p>项目流程：</p><p>1.交易数据接入：</p><p>完成数据源到数仓的数据接入，完成ODS层的交易数据同步。</p><p>2.交易数据明细层模型建设：</p><p>DWD&amp;DWM：按照电商交易业务流程SOP划分完成相应内容类型的数据域建设，梳理清楚模型设计5要素（数据域、粒度、维度、度量、事实），构建总线矩阵，完成明细层转换、解析清洗，并维度退化，合并成大宽表，包括：交易域-订单明细大宽表、交易域-退货明细大宽表。</p><p>3.交易域轻度汇总层建设：</p><p>DWS:根据需求调研口径按照维度，颗粒度，周期跨度完成轻度汇总公共指标模型的建设。</p><p>4.交易域相关用户主题资产建设</p><p>ADS:通过DWS层公共指标组合及DWM明细内容加工完成ADS应用模型开发，建设用户主题资产表，包括：用户基本信息、用户订单统计信息、用户类型、用户价值。</p><h1 id="数仓项目详细"><a href="#数仓项目详细" class="headerlink" title="数仓项目详细"></a>数仓项目详细</h1><p>1.交易数据接入</p><p> 使用maxwell将业务数据增量接入到hdfs中，并使用load进行载入。</p><p><strong>maxwell定义</strong>：</p><ul><li>maxwell是一个Mysql数据库的增量数据捕获工具，通过读取Mysql的binlog日志来捕获数据变化，以Json格式发送到下游系统。</li></ul><p><strong>maxwell底层原理</strong>：</p><ul><li>mysql的binlog 是记录数据库数据变化的日志文件，所有的 INSERT、UPDATE、DELETE 以及对表结构的更改操作（如 ALTER TABLE）都会写入 binlog 中。这使得 binlog 成为数据库增量数据捕获的重要来源。</li></ul><p>使用datax将业务数据全量接入到hdfs中，并使用load进行载入。</p><p><strong>datax和sqoop的区别</strong>：</p><ul><li>sqoop采用mr进行导入导出，datax仅采用单机，datax单机压力大一些。</li><li>sqoop 只可以在关系型数据库和 hadoop 组件之间进行数据迁移，在hadoop组件之间，关系型数据库之间无法进行迁移，而datax可以。</li><li>sqoop 只支持官方提供的指定几种关系型数据库和 hadoop 组件之间的数据交换，datax用户可以自定义插件。</li></ul><p><strong>datax的内部结构</strong>：</p><ul><li>readplugin-&gt;framework-&gt;writeplugin</li><li><strong>FrameWork</strong> 用于连接reader和write，作为两者的数据传输通道，处理缓冲，流控，并发，转换等核心技术问题。</li></ul><p><strong>datax为什么不能代替flume</strong></p><ul><li>flume面向海量的日志文件采集，datax用于数据同步，数据迁移</li></ul><p>ods层的字段要与数据源字段保持一致，不进行处理，命名ods_ {库名}_ {表名}_ {df&#x2F;di}</p><p><strong>项目中flume的source、channel、sink分别是什么类型的</strong>。</p><ul><li>source是kafkaSource，channel类型是file，sink类型是hdfs</li></ul><p><strong>flume拦截器怎么实现</strong>：</p><ul><li>实现Interceptor接口，重写initialize()，intercept(Event event)，close()方法。intercept中取event的body的相关信息，用fastjson赋值给header再返回event。</li></ul><p>2.dwd数据etl</p><ul><li>统一字段名和字段类型，例如code和name，并将code的相关描述从枚举值维护表中退化到dwd中转化为name</li><li>从单字段的json数据中取出需要的数据，例如address字段</li><li>数据分区的改造设计，以日期为分区，接入每一天的ods数据时首先向dwd插入createtime为当天的新数据，再对180天内的数据进行回刷，使数据的更新及时的同步到dwd中</li></ul><p>3.dwm 维度退化  将所有维度join到该表，并用id对同一个数据域中的所有表进行连接</p><p>4.dws 从dwm中进行取数，计算公共指标，对指标进行汇总（不跨域）（统一指标口径、防止指标计算逻辑混乱、减少指标重复计算）</p><p>包括最近n天（1，7，14，30，90）的创建订单数，支付订单数，交易成功订单数，订单未支付量，交易成功订单金额，平台补贴，商家补贴，各类商品订单的订单数，订单金额，收藏数，最大消费单笔金额，最高sku消费金额，折扣订单数量</p><p>5.ads，用户维表join交易域订单域的dws（跨域）</p><p>用户价值：是否为高价值用户，最近90天内最大单笔消费大于8000或最近90天内成功订单数为2-4个，总额为8000，或最近90天内成功订单＞&#x3D;5，最大单笔消费为1200。</p><p>用户类型：</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs less"><span class="hljs-selector-tag">case</span> <span class="hljs-keyword">when</span> <span class="hljs-built_in">MONTHS_BETWEEN</span>(<span class="hljs-built_in">substr</span>(register_time,<span class="hljs-number">1</span>,<span class="hljs-number">10</span>),<span class="hljs-string">&#x27;$&#123;bizdate&#125;&#x27;</span>)&lt;=<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> t4.order_create_cnt_30d=<span class="hljs-number">0</span>              <br>then <span class="hljs-string">&#x27;未购新用户&#x27;</span>             <br><span class="hljs-keyword">when</span> <span class="hljs-built_in">MONTHS_BETWEEN</span>(<span class="hljs-built_in">substr</span>(register_time,<span class="hljs-number">1</span>,<span class="hljs-number">10</span>),<span class="hljs-string">&#x27;$&#123;bizdate&#125;&#x27;</span>)&lt;=<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> t4.order_create_cnt_30d&gt;<span class="hljs-number">0</span>             <br>then <span class="hljs-string">&#x27;已购新用户&#x27;</span>            <br><span class="hljs-keyword">when</span> <span class="hljs-built_in">MONTHS_BETWEEN</span>(<span class="hljs-built_in">substr</span>(register_time,<span class="hljs-number">1</span>,<span class="hljs-number">10</span>),<span class="hljs-string">&#x27;$&#123;bizdate&#125;&#x27;</span>)&gt;<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> t4.order_create_cnt_90d=<span class="hljs-number">0</span>            <br>then <span class="hljs-string">&#x27;未购老用户&#x27;</span>            <br><span class="hljs-keyword">when</span> <span class="hljs-built_in">MONTHS_BETWEEN</span>(<span class="hljs-built_in">substr</span>(register_time,<span class="hljs-number">1</span>,<span class="hljs-number">10</span>),<span class="hljs-string">&#x27;$&#123;bizdate&#125;&#x27;</span>)&gt;<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> t4.order_create_cnt_90d&gt;<span class="hljs-number">0</span>             <br>then <span class="hljs-string">&#x27;已购老用户&#x27;</span>    <br>as user_register_type --STRING COMMENT <span class="hljs-string">&#x27;用户类型&#x27;</span> ,<br></code></pre></td></tr></table></figure><p>业务流程：下单-订单优惠-订单支付-订单物流-订单取消</p><p>表：主订单表，子订单表，订单优惠表，订单支付表，订单申诉状态表，支付表，物流表，物流轨迹表， 取消订单表，订单退款表，取消订单物流表，取消订单优惠表，商品spu信息表、商品sku信息表、退货原因表</p>]]></content>
    
    
    
    <tags>
      
      <tag>数仓项目</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>datawarehouse</title>
    <link href="/2024/11/14/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/%E6%95%B0%E4%BB%93/%E6%95%B0%E4%BB%93%E7%90%86%E8%AE%BA/"/>
    <url>/2024/11/14/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/%E6%95%B0%E4%BB%93/%E6%95%B0%E4%BB%93%E7%90%86%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="数仓知识"><a href="#数仓知识" class="headerlink" title="数仓知识"></a>数仓知识</h1><p>1.什么是数据仓库</p><p>数据仓库是面向主题的（主要用于业务分析），集成的（将多源数据集成在一起），相对稳定的（数据进入到数据仓库中不易发生改变），随时间变化（随着时间发展，数据仓库中存留的数据会越来越多）的数据集合。</p><p>2.数仓建模有哪几种方式</p><p>ER建模（范式建模）、维度建模、Data Vault建模、Anchor建模，其中ER和维度是主流建模，ER建模常用于关系数据库，维度建模常用于数仓建模。</p><p>范式建模和维度建模的优缺点：</p><p>范式建模符合三范式、没有数据冗余，保证了数据的一致性，数据解耦，方便更新，同时也使得表的数量较多，设计起来较为复杂，关联查询较为复杂，速度较慢，维护成本较高，需要在加入表之后仍满足范式规则。</p><p>维度建模通过预计算和聚合实现了较快的查询，维护成本较低，易于扩展，设计复杂度相较于范式建模更低。</p><p>ER建模的三范式</p><p>第一范式：属性不可分割，一个格子里只有一个值</p><p>第二范式：不能存在部分函数依赖，一行内的所有属性必须完全依赖于主键。例如（姓名不完全依赖于学号课名）</p><p>第三范式：不能存在传递函数依赖，一行内的所有属性必须直接依赖于主键。例如（系主任名不直接依赖于学号）</p><p>3.数仓分层</p><p>ODS层</p><p>DWD层</p><p>DIM层</p><p>DWM层</p><p>DWS层</p><p>ADS层</p><p>4.数仓分层的作用：</p><p>1.有更清晰的数据结构</p><p>2.数据血缘追踪</p><p>3.通过中间层的构建减少重复计算</p><p>4.复杂问题简单化</p><p>5.数仓建模的意义</p><ul><li><p>性能：良好的模型能帮我们快速的查找数据，减少数据的I&#x2F;O吞吐</p></li><li><p>成本：减少数据的重复计算</p></li><li><p>效率：改善用户使用数据的体验，提升使用数据的效率</p></li><li><p>改善统计口径的不一致性，减少数据计算错误的可能性</p></li></ul><p>6.如何评价一个数仓建设的好坏</p><ul><li>完善度：汇总数据能直接满足多少查询需求，即应用层访问汇总层数据的查询比例，可以快速地响应业务方的需求。</li><li>复用度：模型被读取用于产生下游模型的平均数量。</li><li>规范度：主题域、分层、命名规范</li><li>稳定性：取数时是否有时效保障</li><li>扩展性：新增加模型时是否会和老模型产生冲突</li><li>准确性：输出的数据指标质量能够保证</li><li>健壮性：业务快速更新不会影响底层模型</li><li>成本低：存储成本、时间成本、资源成本</li></ul><p>7.数仓建模要素概念</p><ul><li>度量是业务产生的一个数值</li><li>事实是一条业务中度量的集合，事实是指在业务过程或分析领域中实际发生的、可度量的事件或情况</li><li>粒度是度量的单位，是事实的细节程度</li><li>维度是描述事实的角度</li></ul><p>8.数据倾斜产生的原因</p><p>在并行处理的数据集中，某一部分的数据显著多于其他部分（Spark的partition，mr的partition），会造成不同分区的数量具有较大的差异，数量大的分区在后续的task执行变慢，影响了整个任务的结束。</p><p>9.如何排查数据倾斜</p><p>（1）Spark：</p><ul><li>查看SparkUI 发现job的执行时间很长，去查询stage中task的执行时间，若task运行时间差异较大，则发生了数据倾斜，否则是资源不够</li><li>去DAG执行计划中寻找Scan查看表名，确认代码位置，查询各key的数量</li></ul><p>（2）Hive :</p><ul><li>运行输出，某个task卡在99%，或查看8088，某个stage运行时间很长，其他的很短</li><li>使用explain 查看sql语句的执行计划，通过8088看哪个stage时间最长，在执行计划中查找其stage中scan对应的表名，然后去sql语句中查找，再查询各key的数量</li></ul><p>10.如何解决数据倾斜-参数</p><p>（1）Hive：</p><ul><li><p>防止小文件引起的数据倾斜，大量的小文件会启动大量的maptask 导致某个节点的maptask过多，节点间的数据量不均，降低运行效率。</p></li><li><p><code>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</code>在map输入之前合并小文件</p></li><li><p><code>set hive.merge.mapfiles=true</code> 在map端输出对小文件进行合并</p></li><li><p><code>set hive.map.aggr=true</code> 在map端进行聚合，combiner，减小map端的压力</p></li><li><p><code>set hive.groupby.skewindata=true</code> 开启负载均衡，开启之后将mr job拆分成两个mr job，第一个mr job 将map端数据随机分布到reduce中，在reduce上进行部分聚合，第二个mr job 根据第一个mr job结果进行group by，以实行负载均衡。</p></li><li><p><code>set hive.auto.convert.join=true;</code>设置 MapJoin 优化自动开启</p></li><li><p><code>set hive.mapjoin.smalltable.filesize=25000000</code> 设置小表不超过多大时开启 mapjoin 优化</p></li><li><p>调整reduce任务数，以调整分区数量，增加并行度（前提是未手动设置分区数量）</p></li><li><p>当map端计算量较大时，增加map任务数</p></li></ul><p>（2）Spark：</p><ul><li><code>spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, &quot;200&quot;)</code> 调整spark的shuffle时的分区数</li><li><code>spark.conf.set(&quot;spark.sql.autoBroadcastJoinThreshold&quot;, &quot;10485760&quot;)</code> 调整spark广播连接时的最大阈值</li></ul><p>11.如何解决数据倾斜-sql写法</p><ul><li>去除无效Key，若不需要保留key值，则对key进行过滤，或直接将left join改为inner join</li><li>去除无效Key，若需要保留Key值，将key随机打散，这里选用非keyid的负值，避免和skuid重复，发生错误关联 <code>left join on on if(t1.sku_id is null，order_id*-1,t1.sku_id) = t2.sku_id</code></li><li>大表join小表，调整广播连接（Spark）&#x2F;MapJoin(Hive)参数，使用hit语法   &#x2F;*+ MAPJOIN(XX) *&#x2F; </li><li>大表join大表，热点key，热门商家id，先取订单量大的商家id，然后将热门商家的订单表和维度表关联得到热门商家信息，接着将与热门商家关联不上的冷门商家与维度表关联得到冷门商家信息，再union all。（这样热key就可以走广播表join了）</li><li>大表join大表，倾斜key较多，倍数扩容，利用orderid%n将订单表打散成n份，维度表复制n份，将订单表的每部分与维度表连接，再最后union all （原理是打散订单表，增加并行度）</li><li>count(distinct) 转换为group by，再先预聚合，再聚合 (group by sid,uid) group by sid</li><li>窗口聚合函数同理，先粗partition，再细partition</li></ul><p>11.小文件如何产生</p><ul><li>动态分区插入数据(Spark2 和MapReduce)产生大量小文件，导致map数量剧增</li><li>reduce数量多，输出文件多</li><li>数据源本身包含很多小文件</li><li>实时数据落Hive也会有小文件</li></ul><p>12.如何解决小文件问题</p><p>预防小文件：</p><ul><li>减少reduce数量</li><li>Distribute by rand()控制每个分区内的数据量平均</li><li>在同步任务后加个Spark3回刷任务</li><li>设置参数</li></ul><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">set</span> hive.merge.<span class="hljs-attribute">mapfiles</span>=<span class="hljs-literal">true</span>;    默认值ture,在Map-only的任务结束时合并小文件。<br><span class="hljs-built_in">set</span> hive.merge.<span class="hljs-attribute">mapredfiles</span>=<span class="hljs-literal">true</span>;    默认值<span class="hljs-literal">false</span>,在Map-Reduce的任务结束时合并小文件。<br><span class="hljs-built_in">set</span> hive.input.<span class="hljs-attribute">format</span>=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;  执行MAP前进行小文件合并<br></code></pre></td></tr></table></figure><p>已有小文件：</p><ul><li>Spark3回刷分区数据</li></ul><p>13.如何确定reduce任务和map任务的数量</p><p>map任务:</p><ul><li>splitSize&#x3D;Min(maxSize,Max(minSize,blockSize))</li><li>按这个splitSize对文件进行分割，最后一个文件小于splitSize的1.1倍就不用分割。</li><li>设置mapred.map.tasks 为一个较大的值(大于default_num&#x3D;输入文件整体大小&#x2F;splitSize)。</li></ul><p>reduce任务：</p><ul><li>min(hive.exec.reducers.max ，输入总数据量&#x2F;hive.exec.reducers.bytes.per.reducer)</li><li>set mapreduce.job.reduces&#x3D;1;</li></ul><p>14.分桶表-分区表区别</p><ul><li>分桶表：clustered by sorted by 与mr过程中的分区类似，按哈希值均匀分配key值</li><li>分区表：旨在更好的管理数据，按某个列值分配文件夹</li></ul><p>15.spark shuffle的优化</p><ul><li>调整map缓冲区大小，避免过多的文件溢写：set(“spark.shuffle.file.buffer”, “64”)</li><li>调整reduce端拉取数据缓冲区大小：set(“spark.reducer.maxSizeInFlight”, “96”)</li><li>调节reduce端拉取数据重试次数：set(“spark.shuffle.io.maxRetries”, “6”)</li><li>调节reduce端拉取数据等待间隔：set(“spark.shuffle.io.retryWait”, “60s”)</li><li>调整SortShuffle排序操作阈值：set(“spark.shuffle.sort.bypassMergeThreshold”, “400”)</li></ul><p>16.spark内存模型</p><p>堆内内存：</p><ul><li><strong>Execution 内存</strong>：主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据</li><li><strong>Storage 内存</strong>：主要用于存储 spark 的 cache 数据，例如RDD的缓存、unroll数据；</li><li><strong>用户内存（User Memory）</strong>：主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息。</li><li><strong>预留内存（Reserved Memory）</strong>：系统预留内存，会用来存储Spark内部对象。</li></ul><p>堆外内存：</p><ul><li>execution 内存和storage内存</li></ul><p>17.mapreduce优化</p><ul><li><p>输入端开启小文件输入合并 CombineHiveInputFormat</p></li><li><p>减少溢写（Spill）次数：调整mapreduce.task.io.sort.mb和mapreduce.map.sort.spill.percent参数值，增大触发Spill的内存上限，减少Spill次数，从而减少磁盘IO。</p></li><li><p>减少合并（Merge）次数：调整mapreduce.task.io.sort.factor参数，增大Merge的文件数目，减少Merge的次数，缩短MapReduce处理时间。</p></li><li><p>先进行Combine处理：在不影响业务逻辑的前提下，先进行Combine处理，可以减少IO操作和提高处理速度。</p></li><li><p>reduce端</p></li><li><p>（1）合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致Map、Reduce任务间竞争资源，造成处理超时等错误。</p><p>（2）设置Map、Reduce共存：调整slowstart.completedmaps参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间。</p><p>（3）规避使用Reduce：因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。</p><p>（4）合理设置Reduce端的Buffer：默认情况下，数据达到一个阈值的时候，Buffer中的数据就会写入磁盘，然后Reduce会从磁盘中获得所有的数据。也就是说，Buffer和Reduce是没有直接关联的，中间多次写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得Buffer中的一部分数据可以直接输送到Reduce，从而减少IO开销：mapred.job.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读Buffer中的数据直接拿给Reduce使用。这样一来，设置Buffer需要内存，读取数据需要内存，Reduce计算也要内存，所以要根据作业的运行情况进行调整。</p></li></ul><p>18.hive&#x2F;hdfs文件格式：</p><ul><li><p>text file</p></li><li><p>sequence file</p></li><li><p>orc</p></li><li><p>parquet</p></li></ul><p>19.orc与parquet优缺点</p><ul><li>orc不支持嵌套，压缩率和读取速度比parquet高 适用于原生数据层</li><li>parquet支持嵌套 适用于高层</li></ul><p>20.orc结构</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-comment">------ Stripe 1 ------</span><br>[<span class="hljs-keyword">Index</span> Data]Stripe 中的每一列会被划分成多个 <span class="hljs-keyword">Row</span> <span class="hljs-keyword">Group</span>，每一个 <span class="hljs-keyword">Row</span> <span class="hljs-keyword">Group</span> 默认是 <span class="hljs-number">10000</span> 行。<span class="hljs-keyword">Row</span> <span class="hljs-keyword">Group</span> 是 ORC 的最小读取单元，每一个 <span class="hljs-keyword">Row</span> <span class="hljs-keyword">Group</span> 都有其对应的统计信息，存放在 <span class="hljs-keyword">Index</span> Data 中。<br>[<span class="hljs-keyword">Row</span> Data]<br>[Stripe Footer] Stripe Footer 里面存储着 <span class="hljs-keyword">Index</span> Data 和 <span class="hljs-keyword">Row</span> Data 中每一个 Stream 的 <span class="hljs-keyword">offset</span>/length，不含列的统计信息。<br><span class="hljs-comment">------ Stripe 1 ------</span><br><br><span class="hljs-comment">------ Stripe N ------</span><br>[<span class="hljs-keyword">Index</span> Data]<br>[<span class="hljs-keyword">Row</span> Data]<br>[Stripe Footer]<br><span class="hljs-comment">------ Stripe N ------</span><br><br><span class="hljs-comment">------ Tail ------</span><br>[Metadata] 每个stripe每列的统计信息<br>[Footer] 每个stripe的元信息，行数，<span class="hljs-keyword">offset</span>，length<br>[PostScript] footer和metadata的<span class="hljs-keyword">offset</span>和length<br>[<span class="hljs-number">1</span> byte PostScript length]<br><span class="hljs-comment">------ Tail ------</span><br></code></pre></td></tr></table></figure><p>21.parquet结构</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">------ Row <span class="hljs-keyword">Group</span> <span class="hljs-title">1</span> ------<br>[ColumnChunk] page1 page2  每个page有pageheader和实际数据<br>------ Row <span class="hljs-keyword">Group</span> <span class="hljs-title">1</span> ------<br><br>------ Row <span class="hljs-keyword">Group</span> <span class="hljs-title">N</span> ------<br>[ColumnChunk]<br>------ Row <span class="hljs-keyword">Group</span> <span class="hljs-title">N</span> ------<br><br>------ Tail ------<br>[FileMetadata] 存放了每个 Row <span class="hljs-keyword">Group</span> <span class="hljs-title">及其 ColumnChunk</span> 的元信息和统计信息。这是因为 Row <span class="hljs-keyword">Group</span> <span class="hljs-title">是没有 footer</span> 结构来承担该 Row <span class="hljs-keyword">Group</span> <span class="hljs-title">中的元信息 ，所以 FileMetadata</span> 就承担了整个文件的所有元信息。<br>[<span class="hljs-number">4</span> byte FileMetadata length]<br>------ Tail ------<br></code></pre></td></tr></table></figure><table><thead><tr><th>ORC</th><th>Parquet</th><th>说明</th></tr></thead><tbody><tr><td>Footer+Metadata</td><td>FileMetadata</td><td>描述每一个 Stripe&#x2F;RowGroup 的元信息和统计信息。</td></tr><tr><td>Stripe</td><td>Row Group</td><td>Stripe 有一个 StripeFooter，而 Row Group 没有。</td></tr><tr><td>Row Group</td><td>Page</td><td>都是最小的读取单位。</td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>数仓知识</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SparkRDD</title>
    <link href="/2024/10/23/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Spark/SparkRDD/"/>
    <url>/2024/10/23/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Spark/SparkRDD/</url>
    
    <content type="html"><![CDATA[<h1 id="SparkRDD概念"><a href="#SparkRDD概念" class="headerlink" title="SparkRDD概念"></a>SparkRDD概念</h1><p>1.采用装饰器设计模式，层层包装，类似于传输管道，本身不保留数据</p><p>2.类似于Java String这样的工具类</p><p>3.Spark基于MR设计，操作文件的类都采用MapReduce设计，因此SparkRDD的分区是依赖MapReduce的切片规则实现的。</p><p>在简单内存数据读取的过程中，分区按三个规则走，首先检查是否规定了任务的分区数，若没有则检查默认分区配置，没有则再采用分配的线程核数（local[])</p><p>在文件数据读取过程中，RDD的实际分区数计算方法：</p><p>1.先按目标分区数给每个分区分配空间</p><p>例如目前需要分类13byte，目标分区数2</p><p>每个分区的splitsize 13&#x2F;2&#x3D;6byte</p><p>剩余空间13%2&#x3D;1&lt;6x0.1&#x3D;0.6 所以需要再设置一个分区(MapReduce中的切片) 总分区数就是3</p><p>根据hadoop的切片规则，若最后一次分区后所剩空间小于等于切片大小的0.1，则将其与最后一个切片压缩在一起，以避免小文件。</p>]]></content>
    
    
    
    <tags>
      
      <tag>spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据开发之路-读书</title>
    <link href="/2024/10/18/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E4%B9%8B%E8%B7%AF-%E8%AF%BB%E4%B9%A6/"/>
    <url>/2024/10/18/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E4%B9%8B%E8%B7%AF-%E8%AF%BB%E4%B9%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="大数据之路"><a href="#大数据之路" class="headerlink" title="大数据之路"></a>大数据之路</h1><h2 id="1-日志数据采集"><a href="#1-日志数据采集" class="headerlink" title="1. 日志数据采集"></a>1. 日志数据采集</h2><h3 id="1-1-浏览器的页面日志采集"><a href="#1-1-浏览器的页面日志采集" class="headerlink" title="1.1 浏览器的页面日志采集"></a>1.1 浏览器的页面日志采集</h3><p>浏览器页面采集往往分为两部分：</p><p>（1）页面浏览日志：分为页面浏览量PV和页面访客数UV</p><p>​将JavaScript脚本植入到HTML文档内，当页面被浏览器解析的过程中自动执行</p><p>​采集后，大多数情况会立即执行发送，发送到日志服务器</p><p>​日志服务端收到日志后立即做出回复</p><p>​服务端解析日志并进行存档</p><p>（2）页面交互日志：记录用户在页面上与控件互动的情况</p><p>​业务方在元数据管理界面注册需要采集日志的交互业务，平台生成代码模板供交互式页面进行嵌入，互动代码和控件一起被响应执行</p><p>​日志采集代码将日志发送到日志服务端，服务端进行简单的转储，不进行解析</p><p>在页面数据采集之后，还需要进行数据清洗。</p><h2 id="1-2-无线客户端的日志采集"><a href="#1-2-无线客户端的日志采集" class="headerlink" title="1.2 无线客户端的日志采集"></a>1.2 无线客户端的日志采集</h2><p>无线客户端的日志采集任务交由采集SDK完成，SDK将用户行为分为不同的事件，如页面事件和控件点击事件</p><p>（1）页面事件日志</p><p>​每条页面日志记录三类信息：1.设备和用户的基本信息 2.被访问的页面信息 3.访问的路径，用户还原用户完整的访问行为</p><p>​SDK通常提供三个端口，一个在页面进入时触发，记录在进入页面时的信息，，一个在退出页面时触发，发送日志，除此之外还提供了拓展信息的接口，用于给页面日志添加相关参数</p><p>​SDK还提供透传参数的机制，当前页面的部分信息，可以传递到下一页面，方便进行行为路径追踪</p><p>（2）控件点击和其他事件日志</p><p>​控件点击逻辑简单，只需要记录所在页面，用户、设备信息、控件信息等。</p><p>​其他事件由用户根据业务场景自定义事件来采集相关信息</p><p>​除此之外，SDK还提供了一些无需触发的采集接口，例如捕获应用崩溃的日志信息</p><p>（3）特殊场景</p><p>​为了平衡日志大小，对于某些如曝光和一些性能技术类日志，推荐使用SDK的聚集功能，避免日志的多次上传，减小服务器压力。</p><p>​app具有明显的回退功能，这会影响SDK的访问路径，所以需要对页面是否存在回退行为进行分析</p><p>（4）H5&amp;Native 日志统一</p><p>​为了避免将嵌入到Native中的H5页面日志与Native日志关联时出现数据丢失，成本过高的问题，需要对二者的日志进行统一处理，本文选用H5向Native归的方式</p><p>（5）设备标识</p><p>​在统计UV时，需要通过设备标识来判断用户的唯一性，采用UTDID。</p><p>（6）日志传输</p><p>​无线客户端日志的上传，不是产生一条发送一条，而是先储存在客户端本地，伺机上传。通过POST请求发送给服务端，服务端经过校验，将数据追加到客户端本地文件中进行存储，并进行维度切分，及当天的日志存储到当天文件中。还对日志进行分流。最后通过消息队列将日志数据推送给下游任务。</p><h2 id="2-3-日志采集挑战"><a href="#2-3-日志采集挑战" class="headerlink" title="2.3 日志采集挑战"></a>2.3 日志采集挑战</h2><p>（1）日志分流和定制处理</p><p>（2）采集与计算一体化设计</p><p>​</p><p>​</p>]]></content>
    
    
    
    <tags>
      
      <tag>读书</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive:初见</title>
    <link href="/2024/10/09/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Hive/Hive-%E5%88%9D%E5%A7%8B%E9%85%8D%E7%BD%AE/"/>
    <url>/2024/10/09/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Hive/Hive-%E5%88%9D%E5%A7%8B%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="Hive-远程连接："><a href="#Hive-远程连接：" class="headerlink" title="Hive 远程连接："></a>Hive 远程连接：</h1><p>Hive安装：可以部署到任意一个节点，不一定在集群上，因为Hive是客户端</p><p>安装Mysql以作为Hive存储元数据的数据库，来替代Hive内置的dubby数据库，为了方便，将mysql安装在windows上，以供远程连接</p><p>创建metastore数据库</p><p>安装mysql-connector-java驱动到&#x2F;opt&#x2F;module&#x2F;hive&#x2F;lib&#x2F;</p><p>创建Hive配置文件hive-site.xml配置hive</p><p>初始化Hive</p><p><code>schematool -initSchema -dbType mysql</code></p><p>对hive的元数据库进行初始化</p><p>​此处出现两次错误，第一次由于MysqlConnectorjava版本错误导致的，第二次是未配置时区导致的</p><p><code>&lt;value&gt;jdbc:mysql://192.168.153.1:3306/metastore?useSSL=false&amp;amp;serverTimezone=UTC&lt;/value&gt;</code></p><p>​配置core-site.xml中的hdfs代理用户</p><p>​配置hive-site.xml中的hiveserver端口号和用户</p><p>配置metastore的独立服务模式</p><p>​只在一个节点配置metastore的文件信息，在其他节点配置metastore的客户端连接信息，一旦配置客户端连接信息，该节点优先选择此种连接hive的方式</p><p>启动hive</p><p>先启动hive metastore</p><p><code>nohup hive –-service metastore &amp;</code></p><p>再启动hiveserver2</p><p><code>nohup hive --service hiveserver2 &amp;</code></p><p>至此hive可以被外界进行访问</p><h1 id="Hive-on-Spark搭建："><a href="#Hive-on-Spark搭建：" class="headerlink" title="Hive  on Spark搭建："></a>Hive  on Spark搭建：</h1><p>1.安装Hadoop</p><p>2.安装Hive，且根据使用的spark版本对官方源码进行预编译</p><p>3.安装spark</p><p>4.更改&#x2F;opt&#x2F;module&#x2F;hive&#x2F;conf&#x2F;hive-site.xml和&#x2F;opt&#x2F;module&#x2F;hive&#x2F;conf&#x2F;spark-default.conf</p><p>5.上传纯净版spark的jar包到hdfs的指定目录上</p><p>6.启动hadoop，启动hive（无需额外启动spark）</p><p>遇到问题：datanode是镜像复制出的，导致其uuid一致，造成了同时只能存活同一个节点且不停切换，造成了hive on spark 无法加载jar包和搜索数据的问题（显示can not obtain block），原因是节点的不同切换导致无法访问hdfs的数据副本（但使用hdfs的webui仍能正常下载数据）</p><p>​</p>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>序列化</title>
    <link href="/2024/10/04/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Hadoop/%E5%BA%8F%E5%88%97%E5%8C%96/"/>
    <url>/2024/10/04/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Hadoop/%E5%BA%8F%E5%88%97%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="Hadoop-MapReduce-的序列化接口"><a href="#Hadoop-MapReduce-的序列化接口" class="headerlink" title="Hadoop : MapReduce 的序列化接口"></a>Hadoop : MapReduce 的序列化接口</h1><p>序列化接口书写流程</p><ol><li>创建Bean类实现Writable接口</li><li>创建无参构造</li><li>重写write，readFields方法</li><li>重写toString方法</li></ol><p>创建FlowBean</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FlowBean</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">Writable</span> &#123;<br><br><br>    <span class="hljs-keyword">private</span> <span class="hljs-type">long</span> upFlow;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">long</span> downFlow;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">long</span> sumFlow;<br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">FlowBean</span> <span class="hljs-params">()</span><br>    &#123;<br><br>    &#125;<br>    <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> <span class="hljs-title function_">getUpFlow</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> upFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setUpFlow</span><span class="hljs-params">(<span class="hljs-type">long</span> upFlow)</span> &#123;<br>        <span class="hljs-built_in">this</span>.upFlow = upFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> <span class="hljs-title function_">getDownFlow</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> downFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setDownFlow</span><span class="hljs-params">(<span class="hljs-type">long</span> downFlow)</span> &#123;<br>        <span class="hljs-built_in">this</span>.downFlow = downFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> <span class="hljs-title function_">getSumFlow</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> sumFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setSumFlow</span><span class="hljs-params">(<span class="hljs-type">long</span> sumFlow)</span> &#123;<br>        <span class="hljs-built_in">this</span>.sumFlow = sumFlow;<br>    &#125;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setSumFlow</span><span class="hljs-params">()</span><br>    &#123;<br>        <span class="hljs-built_in">this</span>.sumFlow = <span class="hljs-built_in">this</span>.downFlow + <span class="hljs-built_in">this</span>.upFlow;<br>    &#125;<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">write</span><span class="hljs-params">(DataOutput dataOutput)</span> <span class="hljs-keyword">throws</span> IOException &#123;<br>        dataOutput.writeLong(upFlow);<br>        dataOutput.writeLong(downFlow);<br>        dataOutput.writeLong(sumFlow);<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">readFields</span><span class="hljs-params">(DataInput dataInput)</span> <span class="hljs-keyword">throws</span> IOException &#123;<br>        <span class="hljs-built_in">this</span>.upFlow = dataInput.readLong();<br>        <span class="hljs-built_in">this</span>.downFlow = dataInput.readLong();<br>        <span class="hljs-built_in">this</span>.sumFlow = dataInput.readLong();<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">toString</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> upFlow + <span class="hljs-string">&quot;\t&quot;</span> + downFlow + <span class="hljs-string">&quot;\t&quot;</span> + sumFlow;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>创建FlowMapper</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FlowMapper</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Mapper</span>&lt;Long, Text,Text,FlowBean&gt; &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">Text</span> <span class="hljs-variable">outK</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>();<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">FlowBean</span> <span class="hljs-variable">outV</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">FlowBean</span>();<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">map</span><span class="hljs-params">(Long key, Text value, Mapper&lt;Long, Text, Text, FlowBean&gt;.Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException &#123;<br>        <span class="hljs-type">String</span> <span class="hljs-variable">line</span> <span class="hljs-operator">=</span> value.toString();<br>        String []split = line.split(<span class="hljs-string">&quot;\t&quot;</span>);<br>        <span class="hljs-type">String</span> <span class="hljs-variable">phone</span> <span class="hljs-operator">=</span> split[<span class="hljs-number">1</span>];<br>        <span class="hljs-type">String</span> <span class="hljs-variable">up</span> <span class="hljs-operator">=</span> split[split.length-<span class="hljs-number">3</span>];<br>        <span class="hljs-type">String</span> <span class="hljs-variable">down</span> <span class="hljs-operator">=</span>split[split.length-<span class="hljs-number">2</span>];<br>        outK.set(phone);<br>        outV.setUpFlow(Long.parseLong(up));<br>        outV.setDownFlow(Long.parseLong(down));<br>        outV.setSumFlow();<br>        context.write(outK,outV);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>创建FlowReducer</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FlowReducer</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Reducer</span>&lt;Text,FlowBean,Text,FlowBean&gt; &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">FlowBean</span> <span class="hljs-variable">outV</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">FlowBean</span>();<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">reduce</span><span class="hljs-params">(Text key, Iterable&lt;FlowBean&gt; values, Reducer&lt;Text, FlowBean, Text, FlowBean&gt;.Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException &#123;<br>        <span class="hljs-type">long</span> totalUp=<span class="hljs-number">0</span>;<br>        <span class="hljs-type">long</span> totalDown=<span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span>(FlowBean value :values)<br>        &#123;<br>            totalUp+= value.getUpFlow();<br>            totalDown+=value.getDownFlow();<br>        &#125;<br>        outV.setUpFlow(totalUp);<br>        outV.setDownFlow(totalDown);<br>        outV.setSumFlow();<br>        context.write(key,outV);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>创建FlowDriver</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FlowDriver</span> &#123;<br><br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;<br>        <span class="hljs-type">Configuration</span> <span class="hljs-variable">configuration</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Configuration</span>();<br>        <span class="hljs-type">Job</span> <span class="hljs-variable">job</span> <span class="hljs-operator">=</span> Job.getInstance(configuration);<br>        job.setMapperClass(FlowMapper.class);<br>        job.setReducerClass(FlowReducer.class);<br>        job.setMapOutputKeyClass(Text.class);<br>        job.setMapOutputValueClass(FlowBean.class);<br>        job.setOutputKeyClass(Text.class);<br>        job.setOutputValueClass(FlowBean.class);<br>        FileInputFormat.setInputPaths(job,<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(args[<span class="hljs-number">0</span>]));<br>        FileOutputFormat.setOutputPath(job,<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(args[<span class="hljs-number">1</span>]));<br>        <span class="hljs-type">Boolean</span> <span class="hljs-variable">result</span> <span class="hljs-operator">=</span> job.waitForCompletion(<span class="hljs-literal">true</span>);<br>        System.exit(result?<span class="hljs-number">0</span>:<span class="hljs-number">1</span>);<br>    &#125;<br><br><br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Hadoop HDFS</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop HDFS</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
