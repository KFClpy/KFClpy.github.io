<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>kafka八股</title>
    <link href="/2024/12/17/kafka%E5%85%AB%E8%82%A1/"/>
    <url>/2024/12/17/kafka%E5%85%AB%E8%82%A1/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数仓项目</title>
    <link href="/2024/12/17/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/%E6%95%B0%E4%BB%93/%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE/"/>
    <url>/2024/12/17/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/%E6%95%B0%E4%BB%93/%E6%95%B0%E4%BB%93%E9%A1%B9%E7%9B%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="数仓项目"><a href="#数仓项目" class="headerlink" title="数仓项目"></a>数仓项目</h1><p>题目：电商交易域模型建设&amp;用户主题资产建设</p><p>项目流程：</p><p>1.交易数据接入：</p><p>完成数据源到数仓的数据接入，完成ODS层的交易数据同步。</p><p>2.交易数据明细层模型建设：</p><p>DWD&amp;DWM：按照电商交易业务流程SOP划分完成相应内容类型的数据域建设，梳理清楚模型设计5要素（数据域、粒度、维度、度量、事实），构建总线矩阵，完成明细层转换、解析清洗，并维度退化，合并成大宽表。</p><h1 id="数仓项目详细"><a href="#数仓项目详细" class="headerlink" title="数仓项目详细"></a>数仓项目详细</h1><p>1.交易数据接入</p><p> 使用maxwell将业务数据增量接入到hdfs中，并使用load进行载入。</p><p><strong>maxwell定义</strong>：</p><ul><li>maxwell是一个Mysql数据库的增量数据捕获工具，通过读取Mysql的binlog日志来捕获数据变化，以Json格式发送到下游系统。</li></ul><p><strong>maxwell底层原理</strong>：</p><ul><li>mysql的binlog 是记录数据库数据变化的日志文件，所有的 INSERT、UPDATE、DELETE 以及对表结构的更改操作（如 ALTER TABLE）都会写入 binlog 中。这使得 binlog 成为数据库增量数据捕获的重要来源。</li></ul><p>使用datax将业务数据全量接入到hdfs中，并使用load进行载入。</p><p><strong>datax和sqoop的区别</strong>：</p><ul><li>sqoop采用mr进行导入导出，datax仅采用单机，datax单机压力大一些。</li><li>sqoop 只可以在关系型数据库和 hadoop 组件之间进行数据迁移，在hadoop组件之间，关系型数据库之间无法进行迁移，而datax可以。</li><li>sqoop 只支持官方提供的指定几种关系型数据库和 hadoop 组件之间的数据交换，datax用户可以自定义插件。</li></ul><p><strong>datax的内部结构</strong>：</p><ul><li>readplugin-&gt;framework-&gt;writeplugin</li><li><strong>FrameWork</strong> 用于连接reader和write，作为两者的数据传输通道，处理缓冲，流控，并发，转换等核心技术问题。</li></ul><p><strong>datax为什么不能代替flume</strong></p><ul><li>flume面向海量的日志文件采集，datax用于数据同步，数据迁移</li></ul><p>ods层的字段要与数据源字段保持一致，不进行处理，命名ods_ {库名}_ {表名}_ {df&#x2F;di}</p><p><strong>项目中flume的source、channel、sink分别是什么类型的</strong>。</p><ul><li>source是kafkaSource，channel类型是file，sink类型是hdfs</li></ul><p><strong>flume拦截器怎么实现</strong>：</p><ul><li>实现Interceptor接口，重写initialize()，intercept(Event event)，close()方法。intercept中取event的body的相关信息，用fastjson赋值给header再返回event。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>数仓项目</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>datawarehouse</title>
    <link href="/2024/11/14/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/%E6%95%B0%E4%BB%93/%E6%95%B0%E4%BB%93%E7%90%86%E8%AE%BA/"/>
    <url>/2024/11/14/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/%E6%95%B0%E4%BB%93/%E6%95%B0%E4%BB%93%E7%90%86%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="数仓知识"><a href="#数仓知识" class="headerlink" title="数仓知识"></a>数仓知识</h1><p>1.什么是数据仓库</p><p>数据仓库是面向主题的（主要用于业务分析），集成的（将多源数据集成在一起），相对稳定的（数据进入到数据仓库中不易发生改变），随时间变化（随着时间发展，数据仓库中存留的数据会越来越多）的数据集合。</p><p>2.数仓建模有哪几种方式</p><p>ER建模（范式建模）、维度建模、Data Vault建模、Anchor建模，其中ER和维度是主流建模，ER建模常用于关系数据库，维度建模常用于数仓建模。</p><p>范式建模和维度建模的优缺点：</p><p>范式建模符合三范式、没有数据冗余，保证了数据的一致性，数据解耦，方便更新，同时也使得表的数量较多，设计起来较为复杂，关联查询较为复杂，速度较慢，维护成本较高，需要在加入表之后仍满足范式规则。</p><p>维度建模通过预计算和聚合实现了较快的查询，维护成本较低，易于扩展，设计复杂度相较于范式建模更低。</p><p>ER建模的三范式</p><p>第一范式：属性不可分割，一个格子里只有一个值</p><p>第二范式：不能存在部分函数依赖，一行内的所有属性必须完全依赖于主键。例如（姓名不完全依赖于学号课名）</p><p>第三范式：不能存在传递函数依赖，一行内的所有属性必须直接依赖于主键。例如（系主任名不直接依赖于学号）</p><p>3.数仓分层</p><p>ODS层</p><p>DWD层</p><p>DIM层</p><p>DWM层</p><p>DWS层</p><p>ADS层</p><p>4.数仓分层的作用：</p><p>1.有更清晰的数据结构</p><p>2.数据血缘追踪</p><p>3.通过中间层的构建减少重复计算</p><p>4.复杂问题简单化</p><p>5.数仓建模的意义</p><p>性能：良好的模型能帮我们快速的查找数据，减少数据的I&#x2F;O吞吐</p><p>成本：减少数据的重复计算</p><p>效率：改善用户使用数据的体验，提升使用数据的效率</p><p>改善统计口径的不一致性，减少数据计算错误的可能性</p><p>6.如何评价一个数仓建设的好坏</p><p>完善度：汇总数据能直接满足多少查询需求，即应用层访问汇总层数据的查询比例，可以快速地响应业务方的需求。</p><p>复用度：模型被读取用于产生下游模型的平均数量。</p><p>规范度：主题域、分层、命名规范</p><p>稳定性：取数时是否有时效保障</p><p>扩展性：新增加模型时是否会和老模型产生冲突</p><p>准确性：输出的数据指标质量能够保证</p><p>健壮性：业务快速更新不会影响底层模型</p><p>成本低：存储成本、时间成本、资源成本</p>]]></content>
    
    
    
    <tags>
      
      <tag>数仓知识</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SparkRDD</title>
    <link href="/2024/10/23/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Spark/SparkRDD/"/>
    <url>/2024/10/23/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Spark/SparkRDD/</url>
    
    <content type="html"><![CDATA[<h1 id="SparkRDD概念"><a href="#SparkRDD概念" class="headerlink" title="SparkRDD概念"></a>SparkRDD概念</h1><p>1.采用装饰器设计模式，层层包装，类似于传输管道，本身不保留数据</p><p>2.类似于Java String这样的工具类</p><p>3.Spark基于MR设计，操作文件的类都采用MapReduce设计，因此SparkRDD的分区是依赖MapReduce的切片规则实现的。</p><p>在简单内存数据读取的过程中，分区按三个规则走，首先检查是否规定了任务的分区数，若没有则检查默认分区配置，没有则再采用分配的线程核数（local[])</p><p>在文件数据读取过程中，RDD的实际分区数计算方法：</p><p>1.先按目标分区数给每个分区分配空间</p><p>例如目前需要分类13byte，目标分区数2</p><p>每个分区的splitsize 13&#x2F;2&#x3D;6byte</p><p>剩余空间13%2&#x3D;1&lt;6x0.1&#x3D;0.6 所以需要再设置一个分区(MapReduce中的切片) 总分区数就是3</p><p>根据hadoop的切片规则，若最后一次分区后所剩空间小于等于切片大小的0.1，则将其与最后一个切片压缩在一起，以避免小文件。</p>]]></content>
    
    
    
    <tags>
      
      <tag>spark</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据开发之路-读书</title>
    <link href="/2024/10/18/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E4%B9%8B%E8%B7%AF-%E8%AF%BB%E4%B9%A6/"/>
    <url>/2024/10/18/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E8%B7%AF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E4%B9%8B%E8%B7%AF-%E8%AF%BB%E4%B9%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="大数据之路"><a href="#大数据之路" class="headerlink" title="大数据之路"></a>大数据之路</h1><h2 id="1-日志数据采集"><a href="#1-日志数据采集" class="headerlink" title="1. 日志数据采集"></a>1. 日志数据采集</h2><h3 id="1-1-浏览器的页面日志采集"><a href="#1-1-浏览器的页面日志采集" class="headerlink" title="1.1 浏览器的页面日志采集"></a>1.1 浏览器的页面日志采集</h3><p>浏览器页面采集往往分为两部分：</p><p>（1）页面浏览日志：分为页面浏览量PV和页面访客数UV</p><p>​将JavaScript脚本植入到HTML文档内，当页面被浏览器解析的过程中自动执行</p><p>​采集后，大多数情况会立即执行发送，发送到日志服务器</p><p>​日志服务端收到日志后立即做出回复</p><p>​服务端解析日志并进行存档</p><p>（2）页面交互日志：记录用户在页面上与控件互动的情况</p><p>​业务方在元数据管理界面注册需要采集日志的交互业务，平台生成代码模板供交互式页面进行嵌入，互动代码和控件一起被响应执行</p><p>​日志采集代码将日志发送到日志服务端，服务端进行简单的转储，不进行解析</p><p>在页面数据采集之后，还需要进行数据清洗。</p><h2 id="1-2-无线客户端的日志采集"><a href="#1-2-无线客户端的日志采集" class="headerlink" title="1.2 无线客户端的日志采集"></a>1.2 无线客户端的日志采集</h2><p>无线客户端的日志采集任务交由采集SDK完成，SDK将用户行为分为不同的事件，如页面事件和控件点击事件</p><p>（1）页面事件日志</p><p>​每条页面日志记录三类信息：1.设备和用户的基本信息 2.被访问的页面信息 3.访问的路径，用户还原用户完整的访问行为</p><p>​SDK通常提供三个端口，一个在页面进入时触发，记录在进入页面时的信息，，一个在退出页面时触发，发送日志，除此之外还提供了拓展信息的接口，用于给页面日志添加相关参数</p><p>​SDK还提供透传参数的机制，当前页面的部分信息，可以传递到下一页面，方便进行行为路径追踪</p><p>（2）控件点击和其他事件日志</p><p>​控件点击逻辑简单，只需要记录所在页面，用户、设备信息、控件信息等。</p><p>​其他事件由用户根据业务场景自定义事件来采集相关信息</p><p>​除此之外，SDK还提供了一些无需触发的采集接口，例如捕获应用崩溃的日志信息</p><p>（3）特殊场景</p><p>​为了平衡日志大小，对于某些如曝光和一些性能技术类日志，推荐使用SDK的聚集功能，避免日志的多次上传，减小服务器压力。</p><p>​app具有明显的回退功能，这会影响SDK的访问路径，所以需要对页面是否存在回退行为进行分析</p><p>（4）H5&amp;Native 日志统一</p><p>​为了避免将嵌入到Native中的H5页面日志与Native日志关联时出现数据丢失，成本过高的问题，需要对二者的日志进行统一处理，本文选用H5向Native归的方式</p><p>（5）设备标识</p><p>​在统计UV时，需要通过设备标识来判断用户的唯一性，采用UTDID。</p><p>（6）日志传输</p><p>​无线客户端日志的上传，不是产生一条发送一条，而是先储存在客户端本地，伺机上传。通过POST请求发送给服务端，服务端经过校验，将数据追加到客户端本地文件中进行存储，并进行维度切分，及当天的日志存储到当天文件中。还对日志进行分流。最后通过消息队列将日志数据推送给下游任务。</p><h2 id="2-3-日志采集挑战"><a href="#2-3-日志采集挑战" class="headerlink" title="2.3 日志采集挑战"></a>2.3 日志采集挑战</h2><p>（1）日志分流和定制处理</p><p>（2）采集与计算一体化设计</p><p>​</p><p>​</p>]]></content>
    
    
    
    <tags>
      
      <tag>读书</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive:初见</title>
    <link href="/2024/10/09/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Hive/Hive-%E5%88%9D%E5%A7%8B%E9%85%8D%E7%BD%AE/"/>
    <url>/2024/10/09/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Hive/Hive-%E5%88%9D%E5%A7%8B%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="Hive-远程连接："><a href="#Hive-远程连接：" class="headerlink" title="Hive 远程连接："></a>Hive 远程连接：</h1><p>Hive安装：可以部署到任意一个节点，不一定在集群上，因为Hive是客户端</p><p>安装Mysql以作为Hive存储元数据的数据库，来替代Hive内置的dubby数据库，为了方便，将mysql安装在windows上，以供远程连接</p><p>创建metastore数据库</p><p>安装mysql-connector-java驱动到&#x2F;opt&#x2F;module&#x2F;hive&#x2F;lib&#x2F;</p><p>创建Hive配置文件hive-site.xml配置hive</p><p>初始化Hive</p><p><code>schematool -initSchema -dbType mysql</code></p><p>对hive的元数据库进行初始化</p><p>​此处出现两次错误，第一次由于MysqlConnectorjava版本错误导致的，第二次是未配置时区导致的</p><p><code>&lt;value&gt;jdbc:mysql://192.168.153.1:3306/metastore?useSSL=false&amp;amp;serverTimezone=UTC&lt;/value&gt;</code></p><p>​配置core-site.xml中的hdfs代理用户</p><p>​配置hive-site.xml中的hiveserver端口号和用户</p><p>配置metastore的独立服务模式</p><p>​只在一个节点配置metastore的文件信息，在其他节点配置metastore的客户端连接信息，一旦配置客户端连接信息，该节点优先选择此种连接hive的方式</p><p>启动hive</p><p>先启动hive metastore</p><p><code>nohup hive –-service metastore &amp;</code></p><p>再启动hiveserver2</p><p><code>nohup hive --service hiveserver2 &amp;</code></p><p>至此hive可以被外界进行访问</p><h1 id="Hive-on-Spark搭建："><a href="#Hive-on-Spark搭建：" class="headerlink" title="Hive  on Spark搭建："></a>Hive  on Spark搭建：</h1><p>1.安装Hadoop</p><p>2.安装Hive，且根据使用的spark版本对官方源码进行预编译</p><p>3.安装spark</p><p>4.更改&#x2F;opt&#x2F;module&#x2F;hive&#x2F;conf&#x2F;hive-site.xml和&#x2F;opt&#x2F;module&#x2F;hive&#x2F;conf&#x2F;spark-default.conf</p><p>5.上传纯净版spark的jar包到hdfs的指定目录上</p><p>6.启动hadoop，启动hive（无需额外启动spark）</p><p>遇到问题：datanode是镜像复制出的，导致其uuid一致，造成了同时只能存活同一个节点且不停切换，造成了hive on spark 无法加载jar包和搜索数据的问题（显示can not obtain block），原因是节点的不同切换导致无法访问hdfs的数据副本（但使用hdfs的webui仍能正常下载数据）</p><p>​</p>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hive</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>序列化</title>
    <link href="/2024/10/04/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Hadoop/%E5%BA%8F%E5%88%97%E5%8C%96/"/>
    <url>/2024/10/04/%E6%95%B0%E4%BB%93%E5%BC%80%E5%8F%91/Hadoop/%E5%BA%8F%E5%88%97%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="Hadoop-MapReduce-的序列化接口"><a href="#Hadoop-MapReduce-的序列化接口" class="headerlink" title="Hadoop : MapReduce 的序列化接口"></a>Hadoop : MapReduce 的序列化接口</h1><p>序列化接口书写流程</p><ol><li>创建Bean类实现Writable接口</li><li>创建无参构造</li><li>重写write，readFields方法</li><li>重写toString方法</li></ol><p>创建FlowBean</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FlowBean</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">Writable</span> &#123;<br><br><br>    <span class="hljs-keyword">private</span> <span class="hljs-type">long</span> upFlow;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">long</span> downFlow;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">long</span> sumFlow;<br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">FlowBean</span> <span class="hljs-params">()</span><br>    &#123;<br><br>    &#125;<br>    <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> <span class="hljs-title function_">getUpFlow</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> upFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setUpFlow</span><span class="hljs-params">(<span class="hljs-type">long</span> upFlow)</span> &#123;<br>        <span class="hljs-built_in">this</span>.upFlow = upFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> <span class="hljs-title function_">getDownFlow</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> downFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setDownFlow</span><span class="hljs-params">(<span class="hljs-type">long</span> downFlow)</span> &#123;<br>        <span class="hljs-built_in">this</span>.downFlow = downFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">long</span> <span class="hljs-title function_">getSumFlow</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> sumFlow;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setSumFlow</span><span class="hljs-params">(<span class="hljs-type">long</span> sumFlow)</span> &#123;<br>        <span class="hljs-built_in">this</span>.sumFlow = sumFlow;<br>    &#125;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setSumFlow</span><span class="hljs-params">()</span><br>    &#123;<br>        <span class="hljs-built_in">this</span>.sumFlow = <span class="hljs-built_in">this</span>.downFlow + <span class="hljs-built_in">this</span>.upFlow;<br>    &#125;<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">write</span><span class="hljs-params">(DataOutput dataOutput)</span> <span class="hljs-keyword">throws</span> IOException &#123;<br>        dataOutput.writeLong(upFlow);<br>        dataOutput.writeLong(downFlow);<br>        dataOutput.writeLong(sumFlow);<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">readFields</span><span class="hljs-params">(DataInput dataInput)</span> <span class="hljs-keyword">throws</span> IOException &#123;<br>        <span class="hljs-built_in">this</span>.upFlow = dataInput.readLong();<br>        <span class="hljs-built_in">this</span>.downFlow = dataInput.readLong();<br>        <span class="hljs-built_in">this</span>.sumFlow = dataInput.readLong();<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">toString</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> upFlow + <span class="hljs-string">&quot;\t&quot;</span> + downFlow + <span class="hljs-string">&quot;\t&quot;</span> + sumFlow;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>创建FlowMapper</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FlowMapper</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Mapper</span>&lt;Long, Text,Text,FlowBean&gt; &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">Text</span> <span class="hljs-variable">outK</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Text</span>();<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">FlowBean</span> <span class="hljs-variable">outV</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">FlowBean</span>();<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">map</span><span class="hljs-params">(Long key, Text value, Mapper&lt;Long, Text, Text, FlowBean&gt;.Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException &#123;<br>        <span class="hljs-type">String</span> <span class="hljs-variable">line</span> <span class="hljs-operator">=</span> value.toString();<br>        String []split = line.split(<span class="hljs-string">&quot;\t&quot;</span>);<br>        <span class="hljs-type">String</span> <span class="hljs-variable">phone</span> <span class="hljs-operator">=</span> split[<span class="hljs-number">1</span>];<br>        <span class="hljs-type">String</span> <span class="hljs-variable">up</span> <span class="hljs-operator">=</span> split[split.length-<span class="hljs-number">3</span>];<br>        <span class="hljs-type">String</span> <span class="hljs-variable">down</span> <span class="hljs-operator">=</span>split[split.length-<span class="hljs-number">2</span>];<br>        outK.set(phone);<br>        outV.setUpFlow(Long.parseLong(up));<br>        outV.setDownFlow(Long.parseLong(down));<br>        outV.setSumFlow();<br>        context.write(outK,outV);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>创建FlowReducer</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FlowReducer</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">Reducer</span>&lt;Text,FlowBean,Text,FlowBean&gt; &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-type">FlowBean</span> <span class="hljs-variable">outV</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">FlowBean</span>();<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">reduce</span><span class="hljs-params">(Text key, Iterable&lt;FlowBean&gt; values, Reducer&lt;Text, FlowBean, Text, FlowBean&gt;.Context context)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException &#123;<br>        <span class="hljs-type">long</span> totalUp=<span class="hljs-number">0</span>;<br>        <span class="hljs-type">long</span> totalDown=<span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span>(FlowBean value :values)<br>        &#123;<br>            totalUp+= value.getUpFlow();<br>            totalDown+=value.getDownFlow();<br>        &#125;<br>        outV.setUpFlow(totalUp);<br>        outV.setDownFlow(totalDown);<br>        outV.setSumFlow();<br>        context.write(key,outV);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>创建FlowDriver</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">FlowDriver</span> &#123;<br><br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;<br>        <span class="hljs-type">Configuration</span> <span class="hljs-variable">configuration</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Configuration</span>();<br>        <span class="hljs-type">Job</span> <span class="hljs-variable">job</span> <span class="hljs-operator">=</span> Job.getInstance(configuration);<br>        job.setMapperClass(FlowMapper.class);<br>        job.setReducerClass(FlowReducer.class);<br>        job.setMapOutputKeyClass(Text.class);<br>        job.setMapOutputValueClass(FlowBean.class);<br>        job.setOutputKeyClass(Text.class);<br>        job.setOutputValueClass(FlowBean.class);<br>        FileInputFormat.setInputPaths(job,<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(args[<span class="hljs-number">0</span>]));<br>        FileOutputFormat.setOutputPath(job,<span class="hljs-keyword">new</span> <span class="hljs-title class_">Path</span>(args[<span class="hljs-number">1</span>]));<br>        <span class="hljs-type">Boolean</span> <span class="hljs-variable">result</span> <span class="hljs-operator">=</span> job.waitForCompletion(<span class="hljs-literal">true</span>);<br>        System.exit(result?<span class="hljs-number">0</span>:<span class="hljs-number">1</span>);<br>    &#125;<br><br><br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Hadoop HDFS</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop HDFS</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
